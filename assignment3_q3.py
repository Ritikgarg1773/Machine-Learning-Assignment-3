# -*- coding: utf-8 -*-
"""Assignment3_Q3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DbTuSkSBNJzYpaP7QAViIQUmtm0ikOGA
"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.utils.data as data
from sklearn.preprocessing import StandardScaler
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from torchvision import models
from sklearn import metrics
from sklearn import decomposition
from sklearn import manifold
from sklearn import model_selection
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
import pickle


train_data=pd.read_csv('/content/gdrive/MyDrive/Dataset/largeTrain.csv',sep=",",header=None)
X_train = train_data.iloc[:,1:]
y_train = train_data.iloc[:,0]
val_data=pd.read_csv('/content/gdrive/MyDrive/Dataset/largeValidation.csv',sep=",",header=None)
X_test = val_data.iloc[:,1:]
y_test = val_data.iloc[:,0]
# train_loader = data.DataLoader(train_data, batch_size=64, shuffle=True)
# val_loader=data.DataLoader(val_data,batch_size=64, shuffle=True)



class MLP(nn.Module):
  def __init__(self,input, hidden, output):
    super().__init__() 
    self.inputLayer = nn.Linear(input,hidden)
    self.outputLayer = nn.Linear(hidden,output)
    self.inputLayer.weights = torch.rand(input,hidden)
    self.outputLayer.weights = torch.rand(output,hidden)

  def forward(self,X):
    X = F.relu(self.inputLayer(X))
    X = F.log_softmax(self.outputLayer(X),dim=1)
    return X

hiddenLayers = [5,20,50,100,200]
batch_size = 100
loss_layer_train = []
loss_layer_val = []
for hidden_unit in hiddenLayers:
  model = MLP(X_train.shape[1],hidden_unit,10)
  cross_entropy = nn.CrossEntropyLoss()
  optimizer = optim.Adam(model.parameters(),lr=0.01)
  for epoch in range(100):
    for i in range(0,X_train.shape[0],batch_size):
      model.zero_grad()
      output = model(torch.tensor(X_train.iloc[i:i+batch_size].values).float())
      # print(y_train.iloc[i:i+batch_size].values)
      loss = cross_entropy(output,torch.tensor(y_train.iloc[i:i+batch_size].values))
      loss.backward()
      optimizer.step()
  output_train = model(torch.tensor(X_train.values).float())
  train_loss = cross_entropy(output_train,torch.tensor(y_train.values))
  # print(train_loss[0])
  loss_layer_train.append(train_loss)
  output_val = model(torch.tensor(X_test.values).float())
  val_loss = cross_entropy(output_val,torch.tensor(y_test.values))
  # print(val_loss[0])
  loss_layer_val.append(val_loss)

fig,ax=plt.subplots()
ax2 = ax.twinx()
# xaxis = [*range(1,101)]
ax.plot(hiddenLayers,loss_layer_train,color="red",label="trainingCE")
ax2.plot(hiddenLayers,loss_layer_val,label="ValidationCE")
plt.show()
#we minimise the loss on training data which can be seen in graoh below, but it can vary in validation loss as seen

learningRate = [0.1,0.01,0.001]
batch_size = 100
loss_layer_train_lr = []
loss_layer_val_lr = []
for lr in learningRate:
  model = MLP(X_train.shape[1],4,10)
  l_train= []
  l_val = []
  cross_entropy = nn.CrossEntropyLoss()
  optimizer = optim.Adam(model.parameters(),lr=lr)
  for epoch in range(100):
    for i in range(0,X_train.shape[0],batch_size):
      model.zero_grad()
      output = model(torch.tensor(X_train.iloc[i:i+batch_size].values).float())
      # print(y_train.iloc[i:i+batch_size].values)
      loss = cross_entropy(output,torch.tensor(y_train.iloc[i:i+batch_size].values))
      loss.backward()
      optimizer.step()
    output_train = model(torch.tensor(X_train.values).float())
    train_loss = cross_entropy(output_train,torch.tensor(y_train.values))
    # print(train_loss)
    l_train.append(train_loss)
    output_val = model(torch.tensor(X_test.values).float())
    val_loss = cross_entropy(output_val,torch.tensor(y_test.values))
    # print(train_loss)
    l_val.append(val_loss)
  loss_layer_train_lr.append(l_train)
  loss_layer_val_lr.append(l_val)

fig,ax=plt.subplots()
ax2 = ax.twinx()
xaxis = [*range(1,101)]
ax.plot(xaxis,loss_layer_train_lr[0],color="red")
ax2.plot(xaxis,loss_layer_val_lr[0])
plt.show()

fig,ax=plt.subplots()
ax2 = ax.twinx()
xaxis = [*range(1,101)]
ax.plot(xaxis,loss_layer_train_lr[1],color="red")
ax2.plot(xaxis,loss_layer_val_lr[1])
plt.show()

fig,ax=plt.subplots()
ax2 = ax.twinx()
xaxis = [*range(1,101)]
ax.plot(xaxis,loss_layer_train_lr[2],color="red")
ax2.plot(xaxis,loss_layer_val_lr[2])
plt.show()